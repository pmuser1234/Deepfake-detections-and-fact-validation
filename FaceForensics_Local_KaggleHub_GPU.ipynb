{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé• FaceForensics++ (C23) Deepfake Detector ‚Äì Local KaggleHub Setup\n",
        "---\n",
        "This notebook lets you run the FaceForensics++ deepfake detector **locally** using your **NVIDIA RTX 4060 GPU**.\n",
        "\n",
        "**Includes:**\n",
        "- Conda environment setup commands\n",
        "- KaggleHub dataset caching code\n",
        "- GPU verification block\n",
        "- Original model training code (unchanged)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# üß± 1Ô∏è‚É£ Conda Environment Setup (run these in Anaconda Prompt)\n",
        "# ===============================================================\n",
        "# conda create -n deepfake python=3.10 -y\n",
        "# conda activate deepfake\n",
        "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\n",
        "# pip install timm tqdm opencv-python pillow numpy pandas kagglehub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset cached at: C:\\Users\\Abhishek\\.cache\\kagglehub\\datasets\\xdxd003\\ff-c23\\versions\\1\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# üì• 2Ô∏è‚É£ KaggleHub Dataset Fetcher & Local Caching\n",
        "# ===============================================================\n",
        "import kagglehub, os\n",
        "\n",
        "dataset_path =kagglehub.dataset_download(\"xdxd003/ff-c23\")\n",
        "os.environ[\"BASE_PATH\"] = dataset_path\n",
        "print(f\"‚úÖ Dataset cached at: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CUDA available: True\n",
            "üéÆ Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# ‚öôÔ∏è 3Ô∏è‚É£ GPU Verification\n",
        "# ===============================================================\n",
        "import torch\n",
        "\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected, training will run on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéû Found 7000 videos in C:\\Users\\Abhishek\\.cache\\kagglehub\\datasets\\xdxd003\\ff-c23\\versions\\1\n",
            "‚úÖ Real: 0 | Fake: 7000\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# üîç 4Ô∏è‚É£ Dataset Verification Utility\n",
        "# ===============================================================\n",
        "import os\n",
        "\n",
        "BASE_PATH = os.getenv(\"BASE_PATH\", \"./FaceForensics++_C23\")\n",
        "\n",
        "def verify_dataset(base_dir):\n",
        "    all_videos = []\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.endswith(\".mp4\"):\n",
        "                all_videos.append(os.path.join(root, f))\n",
        "    print(f\"üéû Found {len(all_videos)} videos in {base_dir}\")\n",
        "    real = len([v for v in all_videos if '/original/' in v.lower()])\n",
        "    fake = len(all_videos) - real\n",
        "    print(f\"‚úÖ Real: {real} | Fake: {fake}\")\n",
        "    return all_videos\n",
        "\n",
        "video_files = verify_dataset(BASE_PATH)\n",
        "if not video_files:\n",
        "    print(\"‚ö†Ô∏è No videos found. Check your dataset path or KaggleHub cache.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Dataset Label Breakdown:\n",
            "Real (0): 1000 videos\n",
            "Fake (1): 6000 videos\n",
            "Total   : 7000 videos\n",
            "\n",
            "‚úÖ Train videos: 5600 | Val videos: 1400\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 181\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m    180\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(); tl,ta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    182\u001b[0m         fr\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m b])\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    183\u001b[0m         ey\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meye\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m b])\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
            "File \u001b[1;32mc:\\Users\\Abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\Abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[10], line 90\u001b[0m, in \u001b[0;36mFaceForensicsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     87\u001b[0m path  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_list[idx]\n\u001b[0;32m     88\u001b[0m label \u001b[38;5;241m=\u001b[39m get_label_from_path(path)\n\u001b[1;32m---> 90\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT))\n\u001b[0;32m     92\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, frame_count)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# üé• 5Ô∏è‚É£ Main Training Script (original, unchanged)\n",
        "# ===============================================================\n",
        "import os, cv2, torch, timm, random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "BASE_PATH = os.getenv(\"BASE_PATH\", \"./FaceForensics++_C23\")\n",
        "FRAME_SIZE = (224, 224)\n",
        "FRAME_SKIP = 15\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_label_from_path(path: str) -> int:\n",
        "    lower = path.lower()\n",
        "    if \"/original/\" in lower:\n",
        "        return 0\n",
        "    elif any(x in lower for x in [\n",
        "        \"face2face\", \"faceswap\", \"deepfakes\",\n",
        "        \"faceshifter\", \"neuraltextures\", \"deepfakedetection\"\n",
        "    ]):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def get_all_videos(base_dir):\n",
        "    video_files = []\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.endswith(\".mp4\"):\n",
        "                video_files.append(os.path.join(root, f))\n",
        "    return sorted(video_files)\n",
        "\n",
        "video_list = get_all_videos(BASE_PATH)\n",
        "labels = [get_label_from_path(v) for v in video_list]\n",
        "count = Counter(labels)\n",
        "print(\"üìä Dataset Label Breakdown:\")\n",
        "print(f\"Real (0): {count[0]} videos\")\n",
        "print(f\"Fake (1): {count[1]} videos\")\n",
        "print(f\"Total   : {len(video_list)} videos\\n\")\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "eye_cascade  = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "def extract_eyes(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    eyes_list = []\n",
        "    for (x,y,w,h) in faces:\n",
        "        roi = frame[y:y+h, x:x+w]\n",
        "        eyes = eye_cascade.detectMultiScale(roi)\n",
        "        for (ex,ey,ew,eh) in eyes:\n",
        "            eyes_list.append(roi[ey:ey+eh, ex:ex+ew])\n",
        "    return eyes_list\n",
        "\n",
        "transform_full_base = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "transform_eye = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_aug_real = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "class FaceForensicsDataset(Dataset):\n",
        "    def __init__(self, video_list, frame_skip=10):\n",
        "        self.video_list = video_list\n",
        "        self.frame_skip = frame_skip\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path  = self.video_list[idx]\n",
        "        label = get_label_from_path(path)\n",
        "\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        target_idx = np.random.randint(0, frame_count)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)\n",
        "        ret, frame = cap.read()\n",
        "        cap.release()\n",
        "        if not ret:\n",
        "            frame = np.zeros((224,224,3), np.uint8)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, FRAME_SIZE)\n",
        "\n",
        "        eyes = extract_eyes(frame)\n",
        "        eye = eyes[0] if len(eyes)>0 else np.zeros((32,32,3),np.uint8)\n",
        "        eye_pil   = Image.fromarray(eye)\n",
        "        frame_pil = Image.fromarray(frame)\n",
        "\n",
        "        frame_t = transform_aug_real(frame_pil) if label == 0 else transform_full_base(frame_pil)\n",
        "        eye_t   = transform_eye(eye_pil)\n",
        "\n",
        "        return {\"frame\": frame_t, \"eye\": eye_t, \"label\": torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "class RegionCNN(nn.Module):\n",
        "    def __init__(self,in_ch=3,out_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch,32,3,1,1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,1,1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,1,1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.fc = nn.Linear(128,out_dim)\n",
        "    def forward(self,x):\n",
        "        if x.ndim==3: x=x.unsqueeze(0)\n",
        "        f=self.net(x).view(x.size(0),-1)\n",
        "        return self.fc(f)\n",
        "\n",
        "class HybridDetector(nn.Module):\n",
        "    def __init__(self, swin_name=\"swin_tiny_patch4_window7_224\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.swin = timm.create_model(swin_name, pretrained=True, num_classes=0)\n",
        "        swin_dim = self.swin.num_features\n",
        "        self.eye_net = RegionCNN(out_dim=128)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(swin_dim+128,512), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "    def forward(self,frame,eye):\n",
        "        s = self.swin(frame)\n",
        "        e = self.eye_net(eye)\n",
        "        x = torch.cat([s,e],1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "dataset = FaceForensicsDataset(video_list, FRAME_SKIP)\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(0.8 * dataset_size)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler   = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=4, sampler=train_sampler, collate_fn=lambda x:[i for i in x if i])\n",
        "val_loader   = DataLoader(dataset, batch_size=4, sampler=val_sampler, collate_fn=lambda x:[i for i in x if i])\n",
        "\n",
        "print(f\"‚úÖ Train videos: {len(train_indices)} | Val videos: {len(val_indices)}\")\n",
        "\n",
        "def accuracy(pred,lab):\n",
        "    _,p=torch.max(pred,1)\n",
        "    return (p==lab).float().mean().item()\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self,patience=5,delta=1e-3):\n",
        "        self.patience, self.delta, self.counter, self.best = patience, delta, 0, np.inf\n",
        "        self.stop=False\n",
        "    def check(self,loss):\n",
        "        if loss < self.best - self.delta:\n",
        "            self.best, self.counter = loss, 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter>=self.patience: self.stop=True\n",
        "\n",
        "model = HybridDetector().to(DEVICE)\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "sch   = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "stopper = EarlyStopper(patience=5)\n",
        "EPOCHS=30\n",
        "best_acc=0\n",
        "\n",
        "for ep in range(EPOCHS):\n",
        "    model.train(); tl,ta=0,0\n",
        "    for b in train_loader:\n",
        "        fr=torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "        ey=torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "        lb=torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        out=model(fr,ey)\n",
        "        loss=crit(out,lb)\n",
        "        loss.backward(); opt.step()\n",
        "        tl+=loss.item(); ta+=accuracy(out,lb)\n",
        "    tl/=len(train_loader); ta/=len(train_loader)\n",
        "\n",
        "    model.eval(); vl,va=0,0\n",
        "    with torch.no_grad():\n",
        "        for b in val_loader:\n",
        "            fr=torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "            ey=torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "            lb=torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "            out=model(fr,ey)\n",
        "            loss=crit(out,lb)\n",
        "            vl+=loss.item(); va+=accuracy(out,lb)\n",
        "    vl/=len(val_loader); va/=len(val_loader)\n",
        "    sch.step()\n",
        "    print(f\"üìÜ Epoch {ep+1}/{EPOCHS} | Train {tl:.4f}/{ta*100:.2f}% | Val {vl:.4f}/{va*100:.2f}%\")\n",
        "\n",
        "    if va>best_acc:\n",
        "        best_acc=va\n",
        "        torch.save(model.state_dict(),\"best_balanced_sampler_model.pth\")\n",
        "    stopper.check(vl)\n",
        "    if stopper.stop:\n",
        "        print(\"‚õî Early stopping.\")\n",
        "        break\n",
        "\n",
        "print(f\"üèÅ Training complete. Best Val Acc = {best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20760268",
      "metadata": {},
      "source": [
        "attempt at model training again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cf16e655",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Dataset Label Breakdown:\n",
            "Real (0): 1000 videos\n",
            "Fake (1): 6000 videos\n",
            "Total   : 7000 videos\n",
            "\n",
            "‚úÖ Train videos: 5600 | Val videos: 1400\n",
            "üìÜ Epoch 1/30 | Train 0.6571/84.77% | Val 0.6365/86.71%\n",
            "üìÜ Epoch 2/30 | Train 0.6428/85.45% | Val 0.6207/86.71%\n",
            "üìÜ Epoch 3/30 | Train 0.6411/85.46% | Val 0.6160/86.71%\n",
            "üìÜ Epoch 4/30 | Train 0.6467/85.46% | Val 0.6195/86.71%\n",
            "üìÜ Epoch 5/30 | Train 0.6412/85.46% | Val 0.6160/86.71%\n",
            "üìÜ Epoch 6/30 | Train 0.6417/85.34% | Val 0.6185/86.71%\n",
            "üìÜ Epoch 7/30 | Train 0.6505/85.38% | Val 0.6240/86.71%\n",
            "üìÜ Epoch 8/30 | Train 0.6371/85.43% | Val 0.6215/86.71%\n",
            "‚õî Early stopping triggered.\n",
            "üèÅ Training complete. Best Val Acc = 86.71%\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# üé• Deepfake Detection Training (Balanced + Augmented)\n",
        "# ===============================================================\n",
        "import os, cv2, torch, timm, random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "BASE_PATH = os.getenv(\"BASE_PATH\", \"./FaceForensics++_C23\")\n",
        "FRAME_SIZE = (224, 224)\n",
        "FRAME_SKIP = 15\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------- LABEL LOGIC ----------------\n",
        "def get_label_from_path(path: str) -> int:\n",
        "    lower = path.lower()\n",
        "    if \"/original/\" in lower:\n",
        "        return 0\n",
        "    elif any(x in lower for x in [\n",
        "        \"face2face\", \"faceswap\", \"deepfakes\",\n",
        "        \"faceshifter\", \"neuraltextures\", \"deepfakedetection\"\n",
        "    ]):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def get_all_videos(base_dir):\n",
        "    video_files = []\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.endswith(\".mp4\"):\n",
        "                video_files.append(os.path.join(root, f))\n",
        "    return sorted(video_files)\n",
        "\n",
        "video_list = get_all_videos(BASE_PATH)\n",
        "labels = [get_label_from_path(v) for v in video_list]\n",
        "count = Counter(labels)\n",
        "print(\"üìä Dataset Label Breakdown:\")\n",
        "print(f\"Real (0): {count[0]} videos\")\n",
        "print(f\"Fake (1): {count[1]} videos\")\n",
        "print(f\"Total   : {len(video_list)} videos\\n\")\n",
        "\n",
        "# ---------------- EYE DETECTION ----------------\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "eye_cascade  = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "def extract_eyes(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    eyes_list = []\n",
        "    for (x,y,w,h) in faces:\n",
        "        roi = frame[y:y+h, x:x+w]\n",
        "        eyes = eye_cascade.detectMultiScale(roi)\n",
        "        for (ex,ey,ew,eh) in eyes:\n",
        "            eyes_list.append(roi[ey:ey+eh, ex:ex+ew])\n",
        "    return eyes_list\n",
        "\n",
        "# ---------------- TRANSFORMS ----------------\n",
        "transform_full_base = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "transform_eye = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_aug_real = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# ---------------- DATASET ----------------\n",
        "class FaceForensicsDataset(Dataset):\n",
        "    def __init__(self, video_list, frame_skip=10):\n",
        "        self.video_list = video_list\n",
        "        self.frame_skip = frame_skip\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path  = self.video_list[idx]\n",
        "        label = get_label_from_path(path)\n",
        "\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        target_idx = np.random.randint(0, frame_count)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)\n",
        "        ret, frame = cap.read()\n",
        "        cap.release()\n",
        "        if not ret:\n",
        "            frame = np.zeros((224,224,3), np.uint8)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, FRAME_SIZE)\n",
        "\n",
        "        eyes = extract_eyes(frame)\n",
        "        eye = eyes[0] if len(eyes)>0 else np.zeros((32,32,3),np.uint8)\n",
        "        eye_pil   = Image.fromarray(eye)\n",
        "        frame_pil = Image.fromarray(frame)\n",
        "\n",
        "        frame_t = transform_aug_real(frame_pil) if label == 0 else transform_full_base(frame_pil)\n",
        "        eye_t   = transform_eye(eye_pil)\n",
        "\n",
        "        return {\"frame\": frame_t, \"eye\": eye_t, \"label\": torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "class RegionCNN(nn.Module):\n",
        "    def __init__(self,in_ch=3,out_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch,32,3,1,1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,1,1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,1,1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.fc = nn.Linear(128,out_dim)\n",
        "    def forward(self,x):\n",
        "        if x.ndim==3: x=x.unsqueeze(0)\n",
        "        f=self.net(x).view(x.size(0),-1)\n",
        "        return self.fc(f)\n",
        "\n",
        "class HybridDetector(nn.Module):\n",
        "    def __init__(self, swin_name=\"swin_tiny_patch4_window7_224\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.swin = timm.create_model(swin_name, pretrained=True, num_classes=0)\n",
        "        swin_dim = self.swin.num_features\n",
        "        self.eye_net = RegionCNN(out_dim=128)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(swin_dim+128,512), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "    def forward(self,frame,eye):\n",
        "        s = self.swin(frame)\n",
        "        e = self.eye_net(eye)\n",
        "        x = torch.cat([s,e],1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ---------------- DATA SPLIT ----------------\n",
        "dataset = FaceForensicsDataset(video_list, FRAME_SKIP)\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(0.8 * dataset_size)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler   = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=4, sampler=train_sampler, collate_fn=lambda x:[i for i in x if i])\n",
        "val_loader   = DataLoader(dataset, batch_size=4, sampler=val_sampler, collate_fn=lambda x:[i for i in x if i])\n",
        "\n",
        "print(f\"‚úÖ Train videos: {len(train_indices)} | Val videos: {len(val_indices)}\")\n",
        "\n",
        "# ---------------- LOSS BALANCING ----------------\n",
        "labels = [get_label_from_path(v) for v in video_list]\n",
        "class_counts = np.bincount(labels)\n",
        "weights = torch.tensor(1.0 / class_counts, dtype=torch.float32).to(DEVICE)\n",
        "crit = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# ---------------- TRAINING CONFIG ----------------\n",
        "model = HybridDetector().to(DEVICE)\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "sch   = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self,patience=5,delta=1e-3):\n",
        "        self.patience, self.delta, self.counter, self.best = patience, delta, 0, np.inf\n",
        "        self.stop=False\n",
        "    def check(self,loss):\n",
        "        if loss < self.best - self.delta:\n",
        "            self.best, self.counter = loss, 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter>=self.patience: self.stop=True\n",
        "\n",
        "stopper = EarlyStopper(patience=5)\n",
        "EPOCHS=30\n",
        "best_acc=0\n",
        "\n",
        "# ---------------- TRAINING LOOP ----------------\n",
        "def accuracy(pred,lab):\n",
        "    _,p=torch.max(pred,1)\n",
        "    return (p==lab).float().mean().item()\n",
        "\n",
        "for ep in range(EPOCHS):\n",
        "    model.train(); tl,ta=0,0\n",
        "    for b in train_loader:\n",
        "        fr=torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "        ey=torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "        lb=torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        out=model(fr,ey)\n",
        "        loss=crit(out,lb)\n",
        "        loss.backward(); opt.step()\n",
        "        tl+=loss.item(); ta+=accuracy(out,lb)\n",
        "    tl/=len(train_loader); ta/=len(train_loader)\n",
        "\n",
        "    model.eval(); vl,va=0,0\n",
        "    with torch.no_grad():\n",
        "        for b in val_loader:\n",
        "            fr=torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "            ey=torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "            lb=torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "            out=model(fr,ey)\n",
        "            loss=crit(out,lb)\n",
        "            vl+=loss.item(); va+=accuracy(out,lb)\n",
        "    vl/=len(val_loader); va/=len(val_loader)\n",
        "    sch.step()\n",
        "    print(f\"üìÜ Epoch {ep+1}/{EPOCHS} | Train {tl:.4f}/{ta*100:.2f}% | Val {vl:.4f}/{va*100:.2f}%\")\n",
        "\n",
        "    if va>best_acc:\n",
        "        best_acc=va\n",
        "        torch.save(model.state_dict(),\"best_balanced_fixed_model.pth\")\n",
        "    stopper.check(vl)\n",
        "    if stopper.stop:\n",
        "        print(\"‚õî Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "print(f\"üèÅ Training complete. Best Val Acc = {best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9ee73017",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Using device: cuda\n",
            "üìä Dataset Label Breakdown:\n",
            "Real (0): 1000 videos\n",
            "Fake (1): 6000 videos\n",
            "Total   : 7000 videos\n",
            "\n",
            "‚öñÔ∏è Class Weights: {np.int64(0): np.float64(3.4313725490196076), np.int64(1): np.float64(0.5852842809364549)}\n",
            "‚úÖ Train videos: 5600 | Val videos: 1400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [15:22<00:00,  1.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 1/30 | Train 0.6555/84.64% | Val 0.6370/85.14%\n",
            "üíæ Saved model with Val Acc: 85.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [11:17<00:00,  2.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 2/30 | Train 0.6405/85.55% | Val 0.6395/85.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [11:10<00:00,  2.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 3/30 | Train 0.6388/85.86% | Val 0.6347/85.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [11:11<00:00,  2.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 4/30 | Train 0.6327/85.86% | Val 0.7416/85.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [11:13<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 5/30 | Train 0.6567/84.89% | Val 0.6518/85.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [11:20<00:00,  2.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 6/30 | Train 0.6373/85.36% | Val 0.6388/85.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [11:25<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 7/30 | Train 0.6318/85.71% | Val 0.6429/85.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [11:25<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÜ Epoch 8/30 | Train 0.6331/85.86% | Val 0.6385/85.14%\n",
            "‚õî Early stopping.\n",
            "üèÅ Training complete. Best Val Acc = 85.14%\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# üé• Deepfake Detection ‚Äì Balanced Hybrid Model Training\n",
        "# ===============================================================\n",
        "\n",
        "import os, cv2, torch, timm, random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "BASE_PATH = os.getenv(\"BASE_PATH\", \"./FaceForensics++_C23\")   # update path if needed\n",
        "FRAME_SIZE = (224, 224)\n",
        "FRAME_SKIP = 15\n",
        "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS     = 30\n",
        "print(f\"üöÄ Using device: {DEVICE}\")\n",
        "\n",
        "# ---------------- LABEL MAPPING ----------------\n",
        "def get_label_from_path(path: str) -> int:\n",
        "    \"\"\"Return 0 for Real videos and 1 for Fake ones.\"\"\"\n",
        "    lower = path.lower()\n",
        "    if \"original\" in lower:\n",
        "        return 0\n",
        "    elif any(x in lower for x in [\n",
        "        \"face2face\", \"faceswap\", \"deepfakes\",\n",
        "        \"faceshifter\", \"neuraltextures\", \"deepfakedetection\"\n",
        "    ]):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def get_all_videos(base_dir):\n",
        "    video_files = []\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.endswith(\".mp4\"):\n",
        "                video_files.append(os.path.join(root, f))\n",
        "    return sorted(video_files)\n",
        "\n",
        "video_list = get_all_videos(BASE_PATH)\n",
        "labels = [get_label_from_path(v) for v in video_list]\n",
        "count = Counter(labels)\n",
        "print(\"üìä Dataset Label Breakdown:\")\n",
        "print(f\"Real (0): {count[0]} videos\")\n",
        "print(f\"Fake (1): {count[1]} videos\")\n",
        "print(f\"Total   : {len(video_list)} videos\\n\")\n",
        "\n",
        "# ---------------- HAAR CASCADE FOR EYES ----------------\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "eye_cascade  = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "def extract_eyes(frame):\n",
        "    \"\"\"Detect eye regions using Haar Cascade.\"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    eyes_list = []\n",
        "    for (x,y,w,h) in faces:\n",
        "        roi = frame[y:y+h, x:x+w]\n",
        "        eyes = eye_cascade.detectMultiScale(roi)\n",
        "        for (ex,ey,ew,eh) in eyes:\n",
        "            eyes_list.append(roi[ey:ey+eh, ex:ex+ew])\n",
        "    return eyes_list\n",
        "\n",
        "# ---------------- TRANSFORMS ----------------\n",
        "transform_frame = transforms.Compose([\n",
        "    transforms.Resize(FRAME_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "transform_eye = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# ---------------- DATASET ----------------\n",
        "class FaceForensicsDataset(Dataset):\n",
        "    def __init__(self, video_list, frame_skip=10):\n",
        "        self.video_list = video_list\n",
        "        self.frame_skip = frame_skip\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path  = self.video_list[idx]\n",
        "        label = get_label_from_path(path)\n",
        "\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        target_idx = np.random.randint(0, frame_count)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)\n",
        "        ret, frame = cap.read()\n",
        "        cap.release()\n",
        "\n",
        "        if not ret:\n",
        "            frame = np.zeros((224,224,3), np.uint8)\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, FRAME_SIZE)\n",
        "\n",
        "        eyes = extract_eyes(frame)\n",
        "        eye = eyes[0] if len(eyes) > 0 else np.zeros((32,32,3), np.uint8)\n",
        "        eye_pil   = Image.fromarray(eye)\n",
        "        frame_pil = Image.fromarray(frame)\n",
        "\n",
        "        frame_t = transform_frame(frame_pil)\n",
        "        eye_t   = transform_eye(eye_pil)\n",
        "\n",
        "        return {\"frame\": frame_t, \"eye\": eye_t, \"label\": torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "dataset = FaceForensicsDataset(video_list, FRAME_SKIP)\n",
        "\n",
        "# ---------------- SPLIT & SAMPLER ----------------\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(0.8 * dataset_size)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "# ---------------- FIXED CLASS WEIGHT CALCULATION ----------------\n",
        "train_labels = np.array([int(labels[i]) for i in train_indices], dtype=int)\n",
        "\n",
        "unique_classes = np.unique(train_labels).astype(int)\n",
        "class_weights = compute_class_weight(class_weight='balanced',\n",
        "                                     classes=unique_classes,\n",
        "                                     y=train_labels)\n",
        "\n",
        "# Map class weights correctly to dataset indices\n",
        "label_to_weight = {cls: w for cls, w in zip(unique_classes, class_weights)}\n",
        "weights = [label_to_weight[int(labels[i])] for i in train_indices]\n",
        "\n",
        "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "\n",
        "print(f\"‚öñÔ∏è Class Weights: {label_to_weight}\")\n",
        "\n",
        "print(f\"‚úÖ Train videos: {len(train_indices)} | Val videos: {len(val_indices)}\")\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "class RegionCNN(nn.Module):\n",
        "    def __init__(self,in_ch=3,out_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch,32,3,1,1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,1,1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,1,1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.fc = nn.Linear(128,out_dim)\n",
        "    def forward(self,x):\n",
        "        if x.ndim==3: x=x.unsqueeze(0)\n",
        "        f=self.net(x).view(x.size(0),-1)\n",
        "        return self.fc(f)\n",
        "\n",
        "class HybridDetector(nn.Module):\n",
        "    def __init__(self, swin_name=\"swin_tiny_patch4_window7_224\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.swin = timm.create_model(swin_name, pretrained=True, num_classes=0)\n",
        "        swin_dim = self.swin.num_features\n",
        "        self.eye_net = RegionCNN(out_dim=128)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(swin_dim+128,512), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "    def forward(self,frame,eye):\n",
        "        s = self.swin(frame)\n",
        "        e = self.eye_net(eye)\n",
        "        x = torch.cat([s,e],1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ---------------- TRAINING UTILS ----------------\n",
        "def accuracy(pred,lab):\n",
        "    _,p=torch.max(pred,1)\n",
        "    return (p==lab).float().mean().item()\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self,patience=5,delta=1e-3):\n",
        "        self.patience, self.delta, self.counter, self.best = patience, delta, 0, np.inf\n",
        "        self.stop=False\n",
        "    def check(self,loss):\n",
        "        if loss < self.best - self.delta:\n",
        "            self.best, self.counter = loss, 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter>=self.patience: self.stop=True\n",
        "\n",
        "# ---------------- INITIALIZE MODEL ----------------\n",
        "model = HybridDetector().to(DEVICE)\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "crit  = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(DEVICE))\n",
        "sch   = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
        "stopper = EarlyStopper(patience=5)\n",
        "\n",
        "best_acc=0\n",
        "\n",
        "# ---------------- TRAIN LOOP ----------------\n",
        "for ep in range(EPOCHS):\n",
        "    model.train(); tl,ta=0,0\n",
        "    for b in tqdm(train_loader, desc=f\"Epoch {ep+1}/{EPOCHS}\"):\n",
        "        fr=torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "        ey=torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "        lb=torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out=model(fr,ey)\n",
        "        loss=crit(out,lb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        tl+=loss.item(); ta+=accuracy(out,lb)\n",
        "    tl/=len(train_loader); ta/=len(train_loader)\n",
        "\n",
        "    model.eval(); vl,va=0,0\n",
        "    with torch.no_grad():\n",
        "        for b in val_loader:\n",
        "            fr=torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "            ey=torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "            lb=torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "            out=model(fr,ey)\n",
        "            loss=crit(out,lb)\n",
        "            vl+=loss.item(); va+=accuracy(out,lb)\n",
        "    vl/=len(val_loader); va/=len(val_loader)\n",
        "    sch.step()\n",
        "\n",
        "    print(f\"üìÜ Epoch {ep+1}/{EPOCHS} | Train {tl:.4f}/{ta*100:.2f}% | Val {vl:.4f}/{va*100:.2f}%\")\n",
        "\n",
        "    if va>best_acc:\n",
        "        best_acc=va\n",
        "        torch.save(model.state_dict(),\"best_balanced_model.pth\")\n",
        "        print(f\"üíæ Saved model with Val Acc: {va*100:.2f}%\")\n",
        "\n",
        "    stopper.check(vl)\n",
        "    if stopper.stop:\n",
        "        print(\"‚õî Early stopping.\")\n",
        "        break\n",
        "\n",
        "print(f\"üèÅ Training complete. Best Val Acc = {best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4554ee83",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages (1.7.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abhishek\\anaconda3\\envs\\deepfake\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Abhishek\\AppData\\Local\\Temp\\ipykernel_7928\\1210463297.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_balanced_sampler_model.pth\", map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded model: best_balanced_sampler_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Validation Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [02:48<00:00,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Evaluation Results ==========\n",
            "Accuracy     : 85.14%\n",
            "Precision    : 85.14%\n",
            "Recall       : 100.00%\n",
            "F1 Score     : 91.98%\n",
            "AUC Score    : 0.5646\n",
            "Confusion Matrix:\n",
            "[[   0  208]\n",
            " [   0 1192]]\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# üì• Load the Trained Model\n",
        "# ===============================================================\n",
        "!pip install scikit-learn\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, roc_auc_score\n",
        ")\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the same architecture\n",
        "model = HybridDetector().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"best_balanced_sampler_model.pth\", map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"‚úÖ Loaded model: best_balanced_sampler_model.pth\")\n",
        "\n",
        "# ===============================================================\n",
        "# üìä Evaluate on Validation Set\n",
        "# ===============================================================\n",
        "val_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    sampler=val_sampler,\n",
        "    collate_fn=lambda x: [i for i in x if i]  # FIXED: ensures each sample is a dict\n",
        ")\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for b in tqdm(val_loader, desc=\"Evaluating Validation Set\"):\n",
        "        fr = torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "        ey = torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "        lb = torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "\n",
        "        outputs = model(fr, ey)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(lb.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "# ===============================================================\n",
        "# üßÆ Compute Metrics\n",
        "# ===============================================================\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec  = recall_score(y_true, y_pred)\n",
        "f1   = f1_score(y_true, y_pred)\n",
        "auc  = roc_auc_score(y_true, y_prob)\n",
        "cm   = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\n========== Evaluation Results ==========\")\n",
        "print(f\"Accuracy     : {acc*100:.2f}%\")\n",
        "print(f\"Precision    : {prec*100:.2f}%\")\n",
        "print(f\"Recall       : {rec*100:.2f}%\")\n",
        "print(f\"F1 Score     : {f1*100:.2f}%\")\n",
        "print(f\"AUC Score    : {auc:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(\"========================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fc49fb29",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Abhishek\\AppData\\Local\\Temp\\ipykernel_1916\\3833160434.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_balanced_fixed_model.pth\", map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded model: best_balanced_fixed_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Validation Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [02:27<00:00,  2.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Evaluation Results ==========\n",
            "Accuracy     : 86.57%\n",
            "Precision    : 86.57%\n",
            "Recall       : 100.00%\n",
            "F1 Score     : 92.80%\n",
            "AUC Score    : 0.5565\n",
            "Confusion Matrix:\n",
            "[[   0  188]\n",
            " [   0 1212]]\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, roc_auc_score\n",
        ")\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the same architecture\n",
        "model = HybridDetector().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"best_balanced_fixed_model.pth\", map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"‚úÖ Loaded model: best_balanced_fixed_model.pth\")\n",
        "\n",
        "# ===============================================================\n",
        "# üìä Evaluate on Validation Set\n",
        "# ===============================================================\n",
        "val_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    sampler=val_sampler,\n",
        "    collate_fn=lambda x: [i for i in x if i]  # FIXED: ensures each sample is a dict\n",
        ")\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for b in tqdm(val_loader, desc=\"Evaluating Validation Set\"):\n",
        "        fr = torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "        ey = torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "        lb = torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "\n",
        "        outputs = model(fr, ey)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(lb.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "# ===============================================================\n",
        "# üßÆ Compute Metrics\n",
        "# ===============================================================\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec  = recall_score(y_true, y_pred)\n",
        "f1   = f1_score(y_true, y_pred)\n",
        "auc  = roc_auc_score(y_true, y_prob)\n",
        "cm   = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\n========== Evaluation Results ==========\")\n",
        "print(f\"Accuracy     : {acc*100:.2f}%\")\n",
        "print(f\"Precision    : {prec*100:.2f}%\")\n",
        "print(f\"Recall       : {rec*100:.2f}%\")\n",
        "print(f\"F1 Score     : {f1*100:.2f}%\")\n",
        "print(f\"AUC Score    : {auc:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(\"========================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b0941efe",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Abhishek\\AppData\\Local\\Temp\\ipykernel_1916\\1512408848.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_balanced_model.pth\", map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded model: best_balanced_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Validation Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [03:48<00:00,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Evaluation Results ==========\n",
            "Accuracy     : 85.14%\n",
            "Precision    : 85.14%\n",
            "Recall       : 100.00%\n",
            "F1 Score     : 91.98%\n",
            "AUC Score    : 0.5561\n",
            "Confusion Matrix:\n",
            "[[   0  208]\n",
            " [   0 1192]]\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, roc_auc_score\n",
        ")\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the same architecture\n",
        "model = HybridDetector().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"best_balanced_model.pth\", map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"‚úÖ Loaded model: best_balanced_model.pth\")\n",
        "\n",
        "# ===============================================================\n",
        "# üìä Evaluate on Validation Set\n",
        "# ===============================================================\n",
        "val_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    sampler=val_sampler,\n",
        "    collate_fn=lambda x: [i for i in x if i]  # FIXED: ensures each sample is a dict\n",
        ")\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for b in tqdm(val_loader, desc=\"Evaluating Validation Set\"):\n",
        "        fr = torch.stack([d[\"frame\"] for d in b]).to(DEVICE)\n",
        "        ey = torch.stack([d[\"eye\"] for d in b]).to(DEVICE)\n",
        "        lb = torch.stack([d[\"label\"] for d in b]).long().to(DEVICE)\n",
        "\n",
        "        outputs = model(fr, ey)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(lb.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "# ===============================================================\n",
        "# üßÆ Compute Metrics\n",
        "# ===============================================================\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec  = recall_score(y_true, y_pred)\n",
        "f1   = f1_score(y_true, y_pred)\n",
        "auc  = roc_auc_score(y_true, y_prob)\n",
        "cm   = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\n========== Evaluation Results ==========\")\n",
        "print(f\"Accuracy     : {acc*100:.2f}%\")\n",
        "print(f\"Precision    : {prec*100:.2f}%\")\n",
        "print(f\"Recall       : {rec*100:.2f}%\")\n",
        "print(f\"F1 Score     : {f1*100:.2f}%\")\n",
        "print(f\"AUC Score    : {auc:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(\"========================================\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deepfake",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
