{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzRWh0wtmp4B",
        "outputId": "1d4d4d23-1ef5-4b10-c76e-9b24774578b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m122.9/803.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q moviepy openai-whisper sentence-transformers transformers google-search-results timm opencv-python pillow matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch, timm, random, tempfile, re\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "from serpapi import GoogleSearch\n",
        "from PIL import Image\n",
        "import whisper\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "FRAME_SIZE = (224, 224)\n",
        "SERPAPI_KEY = \"ac4caebfbaa6418c17f1240774d56fab71495fd7bff2e4f6360957af428fca66\" # Replace with your SerpAPI key\n",
        "MODEL_NAME = \"all-mpnet-base-v2\"\n",
        "VIDEO_KEYWORDS = ['youtube', 'tiktok', 'vimeo', 'bilibili', 'dailymotion']"
      ],
      "metadata": {
        "id": "3GJ_8uKcnbs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "eye_cascade  = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "def extract_eyes(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    eyes_list = []\n",
        "    for (x,y,w,h) in faces:\n",
        "        roi = frame[y:y+h, x:x+w]\n",
        "        eyes = eye_cascade.detectMultiScale(roi)\n",
        "        for (ex,ey,ew,eh) in eyes:\n",
        "            eyes_list.append(roi[ey:ey+eh, ex:ex+ew])\n",
        "    return eyes_list\n",
        "\n",
        "transform_full = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "transform_crop = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class RegionCNN(nn.Module):\n",
        "    def __init__(self,in_ch=3,out_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch,32,3,1,1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,1,1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,1,1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.fc = nn.Linear(128,out_dim)\n",
        "    def forward(self,x):\n",
        "        if x.ndim==3: x=x.unsqueeze(0)\n",
        "        f=self.net(x).view(x.size(0),-1)\n",
        "        return self.fc(f)\n",
        "\n",
        "class HybridDetector(nn.Module):\n",
        "    def __init__(self, swin_name=\"swin_tiny_patch4_window7_224\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.swin = timm.create_model(swin_name, pretrained=True, num_classes=0)\n",
        "        swin_dim = self.swin.num_features\n",
        "        self.eye_net = RegionCNN(out_dim=128)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(swin_dim+128,512), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "    def forward(self,frame,eye):\n",
        "        s = self.swin(frame)\n",
        "        e = self.eye_net(eye)\n",
        "        x = torch.cat([s,e],1)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "iy4k8t8VngYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_deepfake(video_path, model_path=\"best_balanced_sampler_model.pth\", interval=15):\n",
        "    model = HybridDetector().to(DEVICE)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    preds = []\n",
        "    print(f\"üß© Analyzing {frame_count} frames ...\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % interval != 0:\n",
        "            continue\n",
        "\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame_resized = cv2.resize(frame_rgb, FRAME_SIZE)\n",
        "        eyes = extract_eyes(frame_resized)\n",
        "        eye_crop = eyes[0] if len(eyes)>0 else np.zeros((32,32,3), np.uint8)\n",
        "\n",
        "        frame_tensor = transform_full(Image.fromarray(frame_resized)).unsqueeze(0).to(DEVICE)\n",
        "        eye_tensor   = transform_crop(Image.fromarray(eye_crop)).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(frame_tensor, eye_tensor)\n",
        "            pred = torch.argmax(out,1).item()\n",
        "        preds.append(pred)\n",
        "\n",
        "    cap.release()\n",
        "    fake_ratio = preds.count(1)/len(preds)\n",
        "    verdict = \"Fake\" if fake_ratio > 0.5 else \"Real\"\n",
        "    print(f\"üìä Deepfake Check ‚Üí {verdict} ({fake_ratio*100:.1f}% fake frames)\")\n",
        "    return verdict\n"
      ],
      "metadata": {
        "id": "_wqAoUZsnitK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio(video_path):\n",
        "    print(\"üéß Extracting audio from video ...\")\n",
        "    audio_path = os.path.join(tempfile.gettempdir(), \"temp_audio.wav\")\n",
        "    clip = VideoFileClip(video_path)\n",
        "    clip.audio.write_audiofile(audio_path, logger=None)\n",
        "    clip.close()\n",
        "    return audio_path\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    print(\"üó£ Transcribing audio ...\")\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"].strip()"
      ],
      "metadata": {
        "id": "qjimkjXtnk0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_web(query, max_results=10):\n",
        "    params = {\"engine\": \"google\", \"q\": query, \"num\": max_results, \"api_key\": SERPAPI_KEY, \"hl\": \"en\"}\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "\n",
        "    data=[]\n",
        "    if \"organic_results\" in results:\n",
        "        for r in results[\"organic_results\"]:\n",
        "            title=r.get(\"title\",\"\"); link=r.get(\"link\",\"\"); snippet=r.get(\"snippet\",\"\")\n",
        "            if not link or not snippet: continue\n",
        "            if any(v in link.lower() for v in VIDEO_KEYWORDS): continue\n",
        "            data.append({\"title\":title,\"link\":link,\"snippet\":snippet})\n",
        "    return data\n",
        "\n",
        "def fact_check(text, embedder, nli_model):\n",
        "    print(\"üîç Running fact check ...\")\n",
        "    results = search_web(text, max_results=15)\n",
        "    if not results:\n",
        "        print(\"‚ö†Ô∏è No search results found.\")\n",
        "        return \"UNCLEAR\"\n",
        "\n",
        "    texts=[r[\"title\"]+\" \"+r[\"snippet\"] for r in results]\n",
        "    emb_texts=embedder.encode(texts, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    emb_query=embedder.encode(text, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    sims=util.cos_sim(emb_query, emb_texts)[0].cpu().numpy()\n",
        "\n",
        "    ranked=[(r,float(s)) for r,s in zip(results,sims) if s>0.3]\n",
        "    ranked=sorted(ranked,key=lambda x:x[1],reverse=True)[:5]\n",
        "    if not ranked: return \"UNCLEAR\"\n",
        "\n",
        "    entail,contra=[],[]\n",
        "    for r,sim in ranked:\n",
        "        snippet=r[\"snippet\"]\n",
        "        labels=[\"entailment\",\"contradiction\",\"neutral\"]\n",
        "        outputs=nli_model(snippet,labels)\n",
        "        label=outputs[\"labels\"][0].upper()\n",
        "        score=outputs[\"scores\"][0]\n",
        "        if \"ENTAIL\" in label: entail.append(score)\n",
        "        elif \"CONTRADICT\" in label: contra.append(score)\n",
        "\n",
        "    ae=np.mean(entail) if entail else 0\n",
        "    ac=np.mean(contra) if contra else 0\n",
        "    if ae-ac>0.15: verdict=\"TRUE\"\n",
        "    elif ac-ae>0.15: verdict=\"FALSE\"\n",
        "    else: verdict=\"UNCLEAR\"\n",
        "    print(f\"‚úÖ Fact Check Verdict ‚Üí {verdict} (Entail={ae:.2f}, Contra={ac:.2f})\")\n",
        "    return verdict"
      ],
      "metadata": {
        "id": "WE2F6eUUnldq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_video(video_path):\n",
        "    print(f\"\\nüé¨ Processing video: {video_path}\")\n",
        "    df_verdict = detect_deepfake(video_path)\n",
        "    audio = extract_audio(video_path)\n",
        "    transcript = transcribe_audio(audio)\n",
        "    embedder = SentenceTransformer(MODEL_NAME)\n",
        "    nli_model = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
        "    fact_verdict = fact_check(transcript, embedder, nli_model)\n",
        "\n",
        "    if df_verdict==\"Fake\" and fact_verdict==\"FALSE\":\n",
        "        final=\"‚ùå Deepfake and Misleading\"\n",
        "    elif df_verdict==\"Fake\" and fact_verdict!=\"FALSE\":\n",
        "        final=\"‚ö†Ô∏è Deepfake but facts unclear/true\"\n",
        "    elif df_verdict==\"Real\" and fact_verdict==\"FALSE\":\n",
        "        final=\"‚ö†Ô∏è Authentic video but misinformation\"\n",
        "    elif df_verdict==\"Real\" and fact_verdict==\"TRUE\":\n",
        "        final=\"‚úÖ Authentic and factually correct\"\n",
        "    else:\n",
        "        final=\"‚ö†Ô∏è Mixed/Unclear\"\n",
        "\n",
        "    print(\"\\n======================================================\")\n",
        "    print(f\"üéØ FINAL VERDICT ‚Üí {final}\")\n",
        "    print(\"======================================================\")\n",
        "    return {\"deepfake\": df_verdict, \"fact\": fact_verdict, \"final\": final, \"transcript\": transcript}\n"
      ],
      "metadata": {
        "id": "1JYENYgknoOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/news_true.mp4\"  # Replace with your video\n",
        "results = analyze_video(video_path)\n",
        "\n",
        "with open(\"factcheck_result.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== TRANSCRIPT ===\\n\"+results[\"transcript\"]+\"\\n\\n\")\n",
        "    f.write(f\"Deepfake Verdict: {results['deepfake']}\\n\")\n",
        "    f.write(f\"Fact Verdict: {results['fact']}\\n\")\n",
        "    f.write(f\"Final Verdict: {results['final']}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Saved transcript and verdicts to factcheck_result.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCtpoE5EnrL9",
        "outputId": "eeeaea65-0742-4591-8080-f6e0a8b49cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üé¨ Processing video: /content/news_true.mp4\n",
            "üß© Analyzing 126 frames ...\n",
            "üìä Deepfake Check ‚Üí Fake (100.0% fake frames)\n",
            "üéß Extracting audio from video ...\n",
            "üó£ Transcribing audio ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Running fact check ...\n",
            "‚úÖ Fact Check Verdict ‚Üí UNCLEAR (Entail=0.56, Contra=0.61)\n",
            "\n",
            "======================================================\n",
            "üéØ FINAL VERDICT ‚Üí ‚ö†Ô∏è Deepfake but facts unclear/true\n",
            "======================================================\n",
            "\n",
            "‚úÖ Saved transcript and verdicts to factcheck_result.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKwHr7RzAIAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}